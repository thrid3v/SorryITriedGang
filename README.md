# RetailNexus ğŸš€

**A Production-Grade, Schema-Agnostic Retail Data Lakehouse.**

RetailNexus is a hybrid data platform that combines **batch processing** (historical data) with **real-time streaming** (live events). It is designed to be **business-vertical agnostic**, meaning it can adapt to any retail schema (Genera Retail, Bakery, Clothing, etc.) through simple configuration changes, without altering the core pipeline code.

Built with **Python**, **DuckDB**, **FastAPI**, and **React**.

---

## âœ¨ Key Features

*   **Hybrid Ingestion**: seamlessly blends static CSV datasets (e.g., from Kaggle) with live synthetic data streams (Faker).
*   **Schema-Agnostic Design**: The entire pipeline (ingestion -> transformation -> analytics) adapts to the schema defined in `config/business_contexts.json`.
*   **Modern Data Stack**:
    *   **Orchestration**: Custom Python-based orchestrator for managing pipeline cycles.
    *   **Storage**: DuckDB for high-performance OLAP queries on Parquet files.
    *   **Architecture**: Bronze (Raw CSV) â†’ Silver (Cleaned Parquet) â†’ Gold (Star Schema) layers.
*   **Advanced Analytics**:
    *   **SCD Type 2**: Tracks historical changes in user data (slowly changing dimensions).
    *   **Partitioning**: Optimized query performance with Hive-style partitioning.
    *   **AI Analyst**: Natural language to SQL conversion for non-technical users (powered by OpenAI).
*   **Secure API**: JWT-based authentication with Role-Based Access Control (Admin vs. Customer).
*   **Interactive Dashboard**: A beautiful React + TypeScript frontend using Shadcn/UI and Recharts.

---

## ğŸ—ï¸ Architecture

The system follows a medallion architecture pattern:

1.  **Ingestion**: Data lands in `data/raw` as CSVs from two sources:
    *   **Batch**: Historical datasets (e.g., Kaggle).
    *   **Stream**: Real-time events generated by `src/ingestion/generator.py`.
2.  **Bronze (Raw)**: Untouched CSV files.
3.  **Silver (Refined)**: Cleaned, deduplicated, and type-cast data stored as Parquet in `data/silver`.
4.  **Gold (Curated)**: True **Star Schema** with Fact and Dimension tables in `data/gold`.
    *   **Facts**: `fact_transactions`, `fact_inventory`, `fact_shipments`
    *   **Dimensions**: `dim_users`, `dim_products`, `dim_stores`, `dim_dates`

---

## ğŸš€ Quick Start

### Prerequisites

*   **Python 3.9+**
*   **Node.js 16+** & **npm**
*   **Git**

### 1. Clone the Repository

```bash
git clone https://github.com/thrid3v/SorryITriedGang.git
cd SorryITriedGang
```

### 2. Backend Setup

Install Python dependencies:

```bash
pip install -r requirements.txt
```

*(Optional) Setup AI Analyst:*
Create a `.env` file in the root directory and add your OpenAI API Key:
```env
OPENAI_API_KEY=sk-proj-your-key-here
```

### 3. Frontend Setup

Install Node.js dependencies:

```bash
cd frontend
npm install
cd ..
```

### 4. Run Everything (The Easy Way)

Use the **Orchestrator** to start the backend API and run the data pipeline on a schedule:

```bash
python src/orchestrator.py
```

*This will:*
1.  *Start the FastAPI server on port 8000.*
2.  *Run the data generation and transformation pipeline every 5 minutes.*

In a **separate terminal**, start the frontend:

```bash
cd frontend
npm run dev
```

Visit the dashboard at **[http://localhost:5173](http://localhost:5173)**.

---

## ğŸ› ï¸ Manual Usage & Commands

If you prefer to run components individually:

### Data Pipeline

1.  **Generate Data**: Creates synthetic data based on the active config.
    ```bash
    python src/ingestion/generator.py
    ```
2.  **Run Pipeline**: Cleans data and builds the Star Schema.
    ```bash
    python src/transformation/pipeline.py
    ```
    *Or run stages individually:*
    *   `python src/transformation/cleaner.py` (Bronze -> Silver)
    *   `python src/transformation/scd_logic.py` (Process Users SCD)
    *   `python src/transformation/star_schema.py` (Silver -> Gold)

### Backend API

Start the server with hot-reload:

```bash
python -m uvicorn api.main:app --reload --port 8000
```

*   **API Docs**: [http://localhost:8000/docs](http://localhost:8000/docs)
*   **Health Check**: [http://localhost:8000/api/health](http://localhost:8000/api/health)

### Default Credentials

*   **Admin**: `admin` / `admin123`
*   **Customer**: Register a new account via the UI.

---

## ğŸ“‚ Project Structure

```text
RetailNexus/
â”œâ”€â”€ api/                  # FastAPI backend
â”‚   â”œâ”€â”€ main.py           # App entry point
â”‚   â””â”€â”€ auth.py           # JWT Authentication
â”œâ”€â”€ config/               # Configuration files
â”‚   â””â”€â”€ business_contexts.json  # Schema definitions
â”œâ”€â”€ data/                 # Data Lake storage
â”‚   â”œâ”€â”€ raw/              # Bronze Layer (CSV)
â”‚   â”œâ”€â”€ silver/           # Silver Layer (Parquet)
â”‚   â””â”€â”€ gold/             # Gold Layer (Star Schema)
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ frontend/             # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # UI Components (Shadcn)
â”‚   â”‚   â”œâ”€â”€ pages/        # Dashboard, Login, etc.
â”‚   â”‚   â””â”€â”€ services/     # API integration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analytics/        # KPI queries & AI logic
â”‚   â”œâ”€â”€ ingestion/        # Data generators & Kaggle loaders
â”‚   â”œâ”€â”€ transformation/   # ETL Pipeline (DuckDB)
â”‚   â””â”€â”€ orchestrator.py   # Pipeline manager
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## ğŸ¤– AI Analyst Configuration

To enable the "Ask Analyst" feature (Text-to-SQL):

1.  Get an OpenAI API Key.
2.  Create `.env` in the project root.
3.  Add `OPENAI_API_KEY=your_key_here`.
4.  Restart the backend API.

The AI Analyst uses your actual schema to answer questions like *"What is the total revenue by city?"* or *"Show me the top 5 selling products"*.

---

## ğŸ§ª Testing

Run specific test scripts to verify logic:

*   **Test KPIs**: `python src/analytics/kpi_queries.py`
*   **Test AI Analyst**: `python src/analytics/nl_query.py`
*   **Check Schema**: `python check_schema.py`

---

## ğŸ› Troubleshooting

*   **Frontend Connection Refused**: Ensure the backend API is running on port 8000 (`python -m uvicorn api.main:app`).
*   **No Data in Dashboard**: Run `python src/transformation/pipeline.py` to populate the Gold layer.
*   **OpenAI Errors**: Verify your `OPENAI_API_KEY` in the `.env` file.
*   **Pipeline Errors**: If you encounter schema conflicts, try clearing the data directory: `rm -rf data/raw/* data/silver/* data/gold/*` and re-running the generator.
